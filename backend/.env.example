# OpenRouter API Key (optional - for cloud LLM access)
# Get your key at https://openrouter.ai/
OPENROUTER_API_KEY=your_openrouter_api_key_here

# ===== Custom API (OpenAI Compatible) =====
# For connecting to vLLM, LocalAI, LM Studio, or any OpenAI-compatible endpoint
# CUSTOM_API_BASE_URL=http://localhost:8080/v1
# CUSTOM_API_KEY=your_api_key_here
# CUSTOM_API_MODEL=your-model-name

# ===== GPU Configuration =====
# Set to "true", "false", or "auto" (default: auto)
# HEARTMULA_4BIT=auto

# Set to "true" or "false" (default: auto-detected based on VRAM)
# HEARTMULA_SEQUENTIAL_OFFLOAD=auto

# ===== torch.compile (Performance Optimization) =====
# Enable torch.compile for ~2x faster inference on supported GPUs
# Requires PyTorch 2.0+ and Triton (pip install triton or triton-windows)
# Set to "true" or "false" (default: false)
# HEARTMULA_COMPILE=false

# torch.compile mode: "default", "reduce-overhead", or "max-autotune"
# - default: Good balance of compile time and performance
# - reduce-overhead: Faster compilation, slightly less optimal code
# - max-autotune: Best performance, but slowest compilation (recommended for production)
# HEARTMULA_COMPILE_MODE=default

# ===== Model Configuration =====
# Model version to use (default: RL-3B-20260123)
# Options: "3B", "RL-3B-20260123"
# HEARTMULA_VERSION=RL-3B-20260123

# Custom model directory (default: backend/models)
# HEARTMULA_MODEL_DIR=/path/to/your/models
